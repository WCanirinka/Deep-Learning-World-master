

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Papers &mdash; Deep-Learning-World 1.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Courses" href="courses.html" />
    <link rel="prev" title="Introduction" href="../intro/intro.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Foreword</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/intro.html">Introduction</a></li>
</ul>
<p class="caption"><span class="caption-text">Content</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Papers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#models">Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#convolutional-networks">Convolutional Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recurrent-networks">Recurrent Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#autoencoders">Autoencoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generative-models">Generative Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#probabilistic-models">Probabilistic Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#core">Core</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#optimization">Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#representation-learning">Representation Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#understanding-and-transfer-learning">Understanding and Transfer Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reinforcement-learning">Reinforcement Learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#applications">Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#image-recognition">Image Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#object-recognition">Object Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#action-recognition">Action Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#caption-generation">Caption Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#natural-language-processing">Natural Language Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#speech-technology">Speech Technology</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="courses.html">Courses</a></li>
<li class="toctree-l1"><a class="reference internal" href="books.html">Books</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs.html">Blogs</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">Document Credentials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../credentials/CONTRIBUTING.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credentials/CODE_OF_CONDUCT.html">Contributor Covenant Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credentials/LICENSE.html">LICENSE</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Deep-Learning-World</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Papers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com///blob/content/papers.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="papers">
<h1>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h1>
<img alt="../_images/article.jpeg" src="../_images/article.jpeg" />
<p>This chapter is associated with the papers published in deep learning.</p>
<div class="section" id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="convolutional-networks">
<h3>Convolutional Networks<a class="headerlink" href="#convolutional-networks" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><img alt="../_images/convolutional.png" src="../_images/convolutional.png" />
</div></blockquote>
<ul>
<li><p class="first"><strong>Imagenet classification with deep convolutional neural networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Convolutional Neural Networks for Sentence Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1408.5882">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Large-scale Video Classification with Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Deep convolutional neural networks for LVCSR</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6639347/&amp;hl=zh-CN&amp;sa=T&amp;oi=gsb&amp;ct=res&amp;cd=0&amp;ei=KknXWYbGFMbFjwSsyICADQ&amp;scisig=AAGBfm2F0Zlu0ciUwadzshNNm80IQQhuhA">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Face recognition: a convolutional neural-network approach</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/554195/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="recurrent-networks">
<h3>Recurrent Networks<a class="headerlink" href="#recurrent-networks" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><img alt="../_images/Recurrent_neural_network_unfold.svg" src="../_images/Recurrent_neural_network_unfold.svg" /></div></blockquote>
<ul>
<li><p class="first"><strong>An empirical exploration of recurrent network architectures</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v37/jozefowicz15.pdf?utm_campaign=Revue%20newsletter&amp;utm_medium=Newsletter&amp;utm_source=revue">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>LSTM: A search space odyssey</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/7508408/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>On the difficulty of training recurrent neural networks</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v28/pascanu13.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning to forget: Continual prediction with LSTM</strong> :
[<a class="reference external" href="http://digital-library.theiet.org/content/conferences/10.1049/cp_19991218">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="autoencoders">
<h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/Autoencoder_structure.png" src="../_images/Autoencoder_structure.png" />
<ul>
<li><p class="first"><strong>Extracting and composing robust features with denoising autoencoders</strong> :
[<a class="reference external" href="https://dl.acm.org/citation.cfm?id=1390294">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</strong> :
[<a class="reference external" href="http://www.jmlr.org/papers/v11/vincent10a.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Adversarial Autoencoders</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.05644">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Autoencoders, Unsupervised Learning, and Deep Architectures</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Reducing the Dimensionality of Data with Neural Networks</strong> :
[<a class="reference external" href="http://science.sciencemag.org/content/313/5786/504">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="generative-models">
<h3>Generative Models<a class="headerlink" href="#generative-models" title="Permalink to this headline">¶</a></h3>
<img alt="../_images/generative.png" src="../_images/generative.png" />
<ul>
<li><p class="first"><strong>Exploiting generative models discriminative classifiers</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/1520-exploiting-generative-models-in-discriminative-classifiers.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Semi-supervised Learning with Deep Generative Models</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Generative Adversarial Nets</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Generalized Denoising Auto-Encoders as Generative Models</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="probabilistic-models">
<h3>Probabilistic Models<a class="headerlink" href="#probabilistic-models" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Stochastic Backpropagation and Approximate Inference in Deep Generative Models</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1401.4082">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Probabilistic models of cognition: exploring representations and inductive biases</strong> :
[<a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1364661310001129">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>On deep generative models with applications to recognition</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/5995710/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
</div>
<div class="section" id="core">
<h2>Core<a class="headerlink" href="#core" title="Permalink to this headline">¶</a></h2>
<div class="section" id="optimization">
<h3>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1502.03167">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong> :
[<a class="reference external" href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Training Very Deep Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5850-training-very-deep-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Large Scale Distributed Deep Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="representation-learning">
<h3>Representation Learning<a class="headerlink" href="#representation-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.06434">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Representation Learning: A Review and New Perspectives</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6472238/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/6399-infogan-interpretable-representation">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="understanding-and-transfer-learning">
<h3>Understanding and Transfer Learning<a class="headerlink" href="#understanding-and-transfer-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Oquab_Learning_and_Transferring_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Distilling the Knowledge in a Neural Network</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1503.02531">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v32/donahue14.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>How transferable are features in deep neural networks?</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-n%E2%80%A6">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="reinforcement-learning">
<h3>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Human-level control through deep reinforcement learning</strong> :
[<a class="reference external" href="https://www.nature.com/articles/nature14236/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Playing Atari with Deep Reinforcement Learning</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.5602">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Continuous control with deep reinforcement learning</strong> :
[<a href="#id76"><span class="problematic" id="id77">`Paper &lt;https://arxiv.org/abs/1509.02971`_</span></a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Deep Reinforcement Learning with Double Q-Learning</strong> :
[<a class="reference external" href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Dueling Network Architectures for Deep Reinforcement Learning</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1511.06581">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
</div>
<div class="section" id="applications">
<h2>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h2>
<div class="section" id="image-recognition">
<h3>Image Recognition<a class="headerlink" href="#image-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Deep Residual Learning for Image Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Very Deep Convolutional Networks for Large-Scale Image Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1409.1556">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Multi-column Deep Neural Networks for Image Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1202.2745">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>DeepID3: Face Recognition with Very Deep Neural Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1502.00873">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.6034">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Image: Scaling up Image Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/vc/arxiv/papers/1501/1501.02876v1.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="object-recognition">
<h3>Object Recognition<a class="headerlink" href="#object-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>ImageNet Classification with Deep Convolutional Neural Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning Deep Features for Scene Recognition using Places Database</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5349-learning-deep-features">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Scalable Object Detection using Deep Neural Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Erhan_Scalable_Object_Detection_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1312.6229">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>CNN Features Off-the-Shelf: An Astounding Baseline for Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/html/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>What is the best multi-stage architecture for object recognition?</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/5459469/">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="action-recognition">
<h3>Action Recognition<a class="headerlink" href="#action-recognition" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Learning Spatiotemporal Features With 3D Convolutional Networks</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Describing Videos by Exploiting Temporal Structure</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Yao_Describing_Videos_by_ICCV_2015_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Convolutional Two-Stream Network Fusion for Video Action Recognition</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Temporal segment networks: Towards good practices for deep action recognition</strong> :
[<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
<div class="section" id="caption-generation">
<h3>Caption Generation<a class="headerlink" href="#caption-generation" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v37/xuc15.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Chen_Minds_Eye_A_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-40.png" src="../_images/progress-overall-40.png" />
</li>
<li><p class="first"><strong>Generative Adversarial Text to Image Synthesis</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/reed16.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Visual-Semantic Al60ignments for Generating Image Descriptions</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Show and Tell: A Neural Image Caption Generator</strong> :
[<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
</ul>
</div>
<div class="section" id="natural-language-processing">
<h3>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Distributed Representations of Words and Phrases and their Compositionality</strong> :
[<a class="reference external" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Efficient Estimation of Word Representations in Vector Space</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Sequence to Sequence Learning with Neural Networks</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1409.3215.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Neural Machine Translation by Jointly Learning to Align and Translate</strong> :
[<a class="reference external" href="https://arxiv.org/pdf/1409.0473.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Get To The Point: Summarization with Pointer-Generator Networks</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1704.04368">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Attention Is All You Need</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1706.03762">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Convolutional Neural Networks for Sentence Classification</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1408.5882">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
</ul>
</div>
<div class="section" id="speech-technology">
<h3>Speech Technology<a class="headerlink" href="#speech-technology" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first"><strong>Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6296526/">Paper</a>]</p>
<img alt="../_images/progress-overall-100.png" src="../_images/progress-overall-100.png" />
</li>
<li><p class="first"><strong>Towards End-to-End Speech Recognition with Recurrent Neural Networks</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v32/graves14.pdf">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Speech recognition with deep recurrent neural networks</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6638947/">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition</strong> :
[<a class="reference external" href="https://arxiv.org/abs/1507.06947">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/amodei16.html">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
<li><p class="first"><strong>Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin</strong> :
[<a class="reference external" href="http://proceedings.mlr.press/v48/amodei16.html">Paper</a>]</p>
<img alt="../_images/progress-overall-80.png" src="../_images/progress-overall-80.png" />
</li>
<li><p class="first"><strong>A novel scheme for speaker recognition using a phonetically-aware deep neural network</strong> :
[<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6853887/">Paper</a>]</p>
<img alt="../_images/progress-overall-60.png" src="../_images/progress-overall-60.png" />
</li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="courses.html" class="btn btn-neutral float-right" title="Courses" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../intro/intro.html" class="btn btn-neutral" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Amirsina Torfi.
      Last updated on True.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.0',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>